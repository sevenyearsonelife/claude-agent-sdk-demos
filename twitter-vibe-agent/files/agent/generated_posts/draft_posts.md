# 主题
GraphRAG 算法工程师面试题

# 帖子草稿
## Post 1
【面试题】GraphRAG 与向量 RAG 的本质区别是什么？为什么说 GraphRAG 不是"用图做检索"？

考察点：系统架构理解 / 信息流设计 / 场景适配能力
面试频率：高 | 难度：进阶

回答要点：向量 RAG 是检索系统，依赖局部相似度匹配；GraphRAG 是"离线语义蒸馏 + 在线聚合"系统。文本通过图谱转化为结构化表示，再通过社区摘要实现语义压缩。查询时使用社区摘要做 map-reduce，本质是 Query-Focused Summarization（QFS），而非检索。

关键区别：向量 RAG 查询时从原始 chunks 检索，GraphRAG 查询时从已蒸馏的社区摘要聚合，图谱在 query-time 完全冷冻。GraphRAG 牺牲成本换取全局理解能力，适合 sensemaking 场景；向量 RAG 适合局部事实检索。

常见误区：认为 GraphRAG 只是"用图做检索的另一种 RAG"或"知识图谱增强检索"。

追问：社区摘要在查询时的作用是什么？为什么不用原始 text chunks？全局性问题 vs 局部性问题分别适合哪种方案？

区分度：基础层知道 GraphRAG 用图谱；进阶层理解社区摘要机制；专家层理解 QFS 本质和场景权衡。

## Post 2
【面试题】Text Chunks 在 GraphRAG 中的作用与传统 RAG 有何不同？

考察点：数据流理解 / 系统边界认知 / 索引设计
面试频率：中 | 难度：基础

回答要点：在 GraphRAG 中，text chunks 只在索引阶段使用，作为"知识抽取的最小证据单元"。LLM 从 chunks 抽取实体、关系、claims，完成图构建后，chunks 的使命就结束了。查询时完全不用 chunks，而是用社区摘要。

传统 RAG 中 chunks 是检索的直接对象，贯穿始终；GraphRAG 中 chunks 只是原材料，最终产出的是"可读索引"（社区摘要）。这种分离让 GraphRAG 能够实现全局语义聚合，而非局部匹配。

边界条件：chunk 太大会导致前文遗忘（LLM 抽取时漏掉前面的实体），chunk 太小会导致 LLM 调用次数爆炸。论文实验推荐 600 tokens 左右，配合 gleaning 机制提升召回率。

追问：chunk size 如何影响 recall 和成本？为什么不能直接全文建图？self-reflection（gleaning）是如何工作的？

区分度：基础层知道需要分块；进阶层理解 chunk 作为证据锚点；专家层理解 chunk 是"最小可审计输入"和 gleaning 机制。

## Post 3
【面试题】GraphRAG 如何生成层级社区？Leiden 算法的递归流程是怎样的？

考察点：图算法理解 / 递归设计 / 停止条件认知
面试频率：高 | 难度：进阶

回答要点：GraphRAG 使用 Leiden 算法进行模块度优化。流程是递归的：在完整图上跑 Leiden 得到第一层社区；对每个社区的诱导子图（induced subgraph）再次运行 Leiden；直到只能返回 1 个社区或规模低于阈值。

Leiden 比 Louvain 更稳定，能避免"断裂社区"。resolution 参数影响层级深度：resolution 越大，层级越深。停止条件包括：社区规模过小、模块度提升低于阈值、只能返回单个社区。

关键认知：递归社区发现只看图结构（节点和边），不看文本 embedding 或 LLM 语义。层级数量是数据驱动的，不是人工指定的超参数。

追问：什么是诱导子图？resolution 参数如何调优？如何保证连接良好的社区？

区分度：基础层知道用 Leiden；进阶层理解递归流程；专家层理解停止条件、参数调优和诱导子图概念。

## Post 4
【面试题】社区摘要是如何生成的？为什么需要"递归"生成？

考察点：摘要策略 / 层级抽象 / Context window 管理
面试频率：中 | 难度：进阶

回答要点：社区摘要是自底向上递归生成的。叶子社区：按节点度优先级加入元素描述（实体、关系、claims）直到 token limit，生成摘要。高层社区：用子社区摘要替换元素摘要，实现压缩。

这种递归策略形成"从全局到局部"的摘要树。高层摘要提供宏观视角，底层摘要保留细节。查询时可以选择合适层级的摘要，而非原始图元素。

关键设计：context window 限制时需要裁剪元素，优先保留高度节点（重要实体）。论文实验发现 8k tokens 在全面性上普遍优于 16k/32k/64k，原因是避免"lost in the middle"现象。

追问：如何控制摘要长度？摘要和图谱是什么关系？摘要裁剪时如何选择保留哪些元素？

区分度：基础层知道有社区摘要；进阶层理解自底向上生成；专家层理解摘要裁剪策略和 lost in the middle 现象。

## Post 5
【面试题】GraphRAG 查询时的 map-reduce 具体做什么？是多层递归 reduce 吗？

考察点：查询机制理解 / QFS 本质 / 常见误区识别
面试频率：高 | 难度：进阶

回答要点：Map-reduce 不是多层递归 reduce（C0→C1→C2→C3），而是一次查询只选一个层级，跑一次 map-reduce。Map：对每个社区摘要 chunk 并行生成局部回答 + helpfulness 分数（0-100）。Filter：丢弃 helpfulness=0 的答案。Reduce：按 helpfulness 排序，迭代加入高分答案直到 token limit，生成最终回答。

本质是 Query-Focused Summarization（QFS），不是传统检索。map 阶段输入是打乱的社区摘要 chunks，不是原始图。这是 GraphRAG 查询时最吃 token 的阶段。

常见误区：认为 map-reduce 是"多层逐级 reduce"，实际上层级选择在 map 之前就确定了，map-reduce 只在选定层级上跑一次。

追问：helpfulness=0 的答案怎么处理？为什么 map 阶段最吃 token？Packed map vs Per-summary map 有什么区别？

区分度：基础层知道有 map-reduce；进阶层理解并行 map；专家层理解 QFS 本质和优化策略。

## Post 6
【面试题】GraphRAG 查询时如何选择使用哪一层级的社区摘要？

考察点：系统设计 / 路由策略 / 问题类型判断
面试频率：高 | 难度：专家

回答要点：层级选择不是自动决定的，是系统级参数。可以通过固定层级、启发式规则或用户指定选择。全局性问题适合高层级（C0/C1），提供宏观视角；局部性问题适合低层级（C2/C3），保留更多细节。

关键认知：一次查询只选一个层级，不会动态切换。路由策略可以基于问题类型（global vs local）、期望答案长度、token 预算等因素设计。更高级的策略可以用 cheap judge 模型预判，再选择层级。

工程实践：C0 层 token 可能只有 C3 的 2.6-9%，但细节会丢失。向量预筛选可以降低成本，但可能损失全局性。分层路由（先在 C0/C1 做 cheap map，只展开相关子树）是常用优化策略。

追问：如何设计问题→层级的决策表？什么情况下应该让用户指定层级？向量预筛选有什么代价？

区分度：基础层知道有层级；进阶层理解不同层级适用不同问题；专家层理解路由策略设计和成本优化。

## Post 7
【面试题】GraphRAG 是否需要领域专家设计本体？LLM 能否完全自动化？

考察点：本体设计 / 工程权衡 / 自动化边界
面试频率：中 | 难度：进阶

回答要点：LLM 可以自动发现实体和关系，但专家定义"哪些重要"才能得到稳定可用的系统。本体不是"世界真理"，而是"压缩策略"——你希望什么被压缩到一起。

优秀 GraphRAG 通常实体类型不超过 10 个，关系类型不超过 20 个。关系类型失控会导致社区结构崩坏（如"导致/影响/关联/提及"混用）。LLM 默认行为是"覆盖最大化"，但工程需要"有意义的压缩"。

需要专家介入的场景：全局性问题、社区可解释性要求高、需要评估复现时。实体类型调整、关系合并、摘要模板变化会触发部分或全部 rebuild。

追问：本体太宽泛会有什么问题？关系类型失控会如何影响社区结构？Schema 变更的影响范围是什么？

区分度：基础层知道需要定义实体类型；进阶层理解本体影响社区质量；专家层理解"本体=压缩策略"和 rebuild 成本。

## Post 8
## Thread 1/2
【面试题】为什么 GraphRAG "吃 token"？有哪些优化策略？

考察点：成本分析 / 优化策略 / 工程权衡
面试频率：高 | 难度：进阶

回答要点：GraphRAG 主要成本在两个阶段。Build 阶段：大量 LLM 调用用于实体抽取、关系抽取、claims 抽取、社区摘要生成。Global query 阶段：map 阶段可能需要处理大量社区摘要，每个摘要都要跑一次 LLM。

Token 消耗对比（Podcast 数据集）：C0 层 26,657 tokens（2.6%），C3 层 746,100 tokens（73.5%），直接文本摘要 1,014,611 tokens（100%）。C0 比直接摘要省 97%+ token，但细节丢失。

优化策略包括：分层路由（先在 C0/C1 做 cheap map）；两段式 map（小模型判相关，大模型生成）；Packed map（多个摘要打包进一次 LLM 调用）；缓存（map 输出和 routing 结果复用）；向量预筛选（top-k 社区摘要，但可能损失全局性）。

追问：向量预筛选会省 token 但有什么代价？Packed map vs Per-summary map 的区别是什么？

## Thread 2/2
工程权衡：每种优化都有代价。向量预筛选可能破坏全局性；Packed map 可能降低并行度；分层路由增加系统复杂度。需要根据场景选择：全局性问题必须避免预筛选；高频查询场景优先考虑缓存；成本敏感场景可以用小模型做 routing。

关键认知：GraphRAG 的成本是设计取舍的结果，不是 bug。它牺牲成本换取全局理解能力，这是向量 RAG 无法提供的。优化目标是"在保证全局性的前提下降低成本"，而不是"为了便宜牺牲核心能力"。

区分度：基础层知道 GraphRAG 贵；进阶层理解 build vs inference 成本；专家层理解各种优化策略及其权衡。

## Post 9
【面试题】如何评估 GraphRAG 的效果？为什么不能用传统 QA 数据集？

考察点：评估方法 / 指标设计 / LLM-as-a-judge
面试频率：中 | 难度：进阶

回答要点：论文使用四个维度：全面性（Comprehensiveness）——是否覆盖所有相关方面；多样性（Diversity）——是否涵盖不同视角；赋能性（Empowerment）——是否帮助理解；直接性（Directness）——是否简洁直接。

通过 LLM-as-a-judge 做头对头比较。生成 corpus-specific 的全局性问题，不依赖 ground truth。这是自适应基准生成方法，比固定 QA 数据集更能反映 GraphRAG 的全局理解能力。

传统 QA 数据集不适合的原因：GraphRAG 的核心能力是全局 sensemaking，传统数据集多为局部事实问答。传统检索指标（recall@k、precision@k）无法评估聚合质量和全局理解。

追问：claim-based 指标如何计算？LLM-as-a-judge 如何保证公平性？如何生成 corpus-specific 的全局性问题？

区分度：基础层知道有四个指标；进阶层理解 LLM-as-a-judge 方法；专家层理解自适应基准生成。

## Post 10
【面试题】GraphRAG 是否只支持全局性问题？Local 查询如何实现？

考察点：查询类型理解 / 系统设计 / 场景适配
面试频率：中 | 难度：进阶

回答要点：GraphRAG 的核心设计目标是 global 问题，但也支持 local 查询。Global 查询：使用全量社区摘要跑 map-reduce，适合"整个语料的主要主题是什么"这类问题。Local 查询：可以用子图、局部社区摘要，不需要跑全量 map-reduce。

Local 查询和 Vector RAG 的 local 有本质区别：Vector RAG 的 local 是基于相似度的 chunk 检索；GraphRAG 的 local 是基于图结构的局部社区聚合。GraphRAG 的 local 仍然保留一定的结构化理解能力。

关键认知：Local 问题走 GraphRAG 可能不如 Vector RAG 经济。理想的设计是 hybrid：global 问题走 GraphRAG，local 问题走 Vector RAG，通过 router 决策。路由可以基于问题关键词、长度、用户偏好等。

追问：local 查询和 vector RAG 的 local 有什么区别？何时应该用哪种？如何设计 hybrid 系统？

区分度：基础层知道 GraphRAG 擅长全局问题；进阶层理解 local/global 查询路径；专家层理解 hybrid 设计和路由策略。

## Post 11
【面试题】如何从 text chunks 抽取实体并解决同一实体多种指代的问题？

考察点：实体抽取 / 实体对齐 / Gleaning 机制
面试频率：中 | 难度：进阶

回答要点：LLM 从每个 chunk 抽取实体、关系、claims。实体对齐通过 exact string matching 或 embedding/LLM 软匹配。GraphRAG 对重复实体有一定鲁棒性——重复实体会被聚类到同一个社区。

Gleaning（self-reflection）机制：LLM 抽取实体后评估是否遗漏，如有遗漏则继续抽取。效果显著：600 token chunk 比 2400 token 抽取的实体几乎多一倍（加 gleaning 后接近）。这允许使用更大的 chunk size 而不牺牲召回率。

实体对齐实践：先规则（别名表、章节邻近），再用 embedding/LLM 软匹配复核。别名、化名、变身关系需要特别处理（如《西游记》中的孙悟空=齐天大圣=弼马温）。

追问：gleaning 是如何工作的？为什么 chunk 越大抽取的实体越少？如何处理别名和化名？

区分度：基础层知道需要抽取实体；进阶层理解实体对齐；专家层理解 gleaning 机制和召回优化。

## Post 12
## Thread 1/2
【面试题】Claims（主张）在 GraphRAG 中起什么作用？

考察点：Claims 机制 / 可解释性设计 / 评估支持
面试频率：低 | 难度：专家

回答要点：Claims 是"可验证的事实陈述"，作为图的协变量（covariates）。每个 claim 必须带证据指针（chapter + span），支持可解释性和可追溯性。Claims 挂到实体/关系上，参与社区摘要生成。

Claims 的三个作用：参与社区摘要生成（作为元素描述的一部分）；支持可追溯性（可以追踪结论的原始来源）；支持 claim-based 评估（评估答案是否基于原文）。

关键认知：没有 claims 就无法回答"结论从哪儿来"。Claims 不是可选的附加信息，而是 GraphRAG 可解释性的核心。传统 RAG 可以返回检索的 chunks，但 GraphRAG 的结论是聚合生成的，必须通过 claims 实现可追溯。

## Thread 2/2
Claims 在评估中的应用：claim-based 指标可以评估生成答案是否忠实于原文，是否有幻觉。LLM-as-a-judge 可以检查答案中的每个 claim 是否能在原文中找到支持。

工程实践：claims 抽取需要额外 LLM 调用，增加成本。可以权衡抽取粒度：只对重要实体/关系抽取 claims，或用规则辅助抽取。claims 的存储和索引也需要考虑——需要通过 community_id 反查到对应的 claim_ids。

追问：claims 如何参与社区摘要生成？如何用于 claim-based 评估？如何优化 claims 抽取成本？

区分度：基础层知道有 claims；进阶层理解 claims 作为证据锚点；专家层理解 claims 在评估和可追溯性中的作用。

## Post 13
【面试题】如何从更高层次理解 GraphRAG？它和 KG-RAG 的本质区别是什么？

考察点：抽象能力 / 本质理解 / 系统分类
面试频率：低 | 难度：专家

回答要点：GraphRAG 是"两次信息重表达"：文本→图（结构化、去冗余）；图→社区摘要（语义压缩、主题化）。图谱是"主题边界生成器"，社区摘要是"可读索引"。

与传统 KG-RAG 的区别：KG-RAG 通常用图谱做检索增强，查询时在图上漫游；GraphRAG 的图谱在 query-time 完全冷冻，查询只读社区摘要。GraphRAG 不是检索系统，而是"离线语义蒸馏 + 在线聚合"系统。

从信息论视角：GraphRAG 是一种"有损压缩 + 查询时解压"的策略。图谱是中间表示，社区摘要是压缩后的语义缓存。查询时在缓存上聚合，而非在原始数据上检索。

追问：为什么图谱在 query-time 可以"完全冷冻"？社区摘要为什么不是图的一部分？如何理解 GraphRAG 的信息论本质？

区分度：基础层知道 GraphRAG 用图；进阶层理解两阶段重表达；专家层理解信息论/压缩视角和与 KG-RAG 的本质区别。

## Post 14
## Thread 1/2
【面试题】递归社区发现的实现细节是什么？什么是诱导子图？

考察点：图算法实现 / 递归设计 / 数据结构理解
面试频率：中 | 难度：进阶

回答要点：递归社区发现的具体实现：在完整图上跑 Leiden→得到第一层社区；对每个社区取诱导子图（包含该社区所有节点及这些节点之间的所有边）；在诱导子图上再跑 Leiden；直到只能返回 1 个社区或规模低于阈值。

诱导子图的概念：给定节点集合，诱导子图包含这些节点及它们之间的所有边。递归社区发现就是对每个社区取其诱导子图，再在子图上运行社区发现。

Leiden 如何保证连接良好的社区：Leiden 通过移动节点到相邻社区来优化模块度，保证每个社区内部连接密集、外部连接稀疏。比 Louvain 更稳定，能避免"断裂社区"。

## Thread 2/2
Resolution 参数的影响：resolution 越大，模块度计算中对社区内边的权重惩罚越小，算法倾向于生成更多、更小的社区→层级更深。resolution 越小，倾向于生成更少、更大的社区→层级更浅。

停止条件：只能返回 1 个社区（无法再分）；社区规模低于阈值；模块度提升不足（继续细分没有意义）。

关键认知：递归社区发现只看图结构，不看文本 embedding 或 LLM 语义。层级数量是数据驱动的，由图结构、resolution 参数、停止条件共同决定。

追问：什么是诱导子图？resolution 参数如何调优？如何评估社区划分质量？

区分度：基础层知道用 Leiden；进阶层理解递归流程；专家层理解诱导子图、resolution 参数和停止条件。

## Post 15
【面试题】社区摘要存储在哪里？和图谱是什么关系？

考察点：存储设计 / 系统架构 / 可追溯性
面试频率：低 | 难度：进阶

回答要点：社区摘要不是图节点，而是图的"派生索引层"。可存储在向量数据库、文档库或文件系统。通过 community_id 反查到 node_ids/edge_ids/claim_ids。

摘要和图的关系：摘要是派生的、独立的索引层，不是图的一部分。摘要使用 community_id 作为主键，通过映射表关联到图元素。摘要和元素不是一一对应的——摘要可能只使用社区的一部分元素（因为 context window 裁剪）。

存储选择：如果需要摘要检索，可以存入向量数据库；如果只需要 ID 查询，可以存入文档库或 KV 存储。关键是要维护 community_id 到图元素的双向映射，支持可追溯性。

追问：如何从摘要反查到对应的图元素？摘要和元素是一一对应吗？context window 裁剪时如何选择保留哪些元素？

区分度：基础层知道有社区摘要；进阶层理解摘要和图的分离；专家层理解可追溯性设计和存储权衡。
