# 主题
算法工程师岗位的 Codex vs Claude Code 对比

# 帖子草稿
## Post 1

【面试题】Claude Code 与 Codex 的核心架构差异是什么？请从 Agent 循环设计、工具哲学和并行控制三个维度进行对比。

| 考察点 | 面试频率 | 难度 |
| --- | --- | --- |
| Agent 循环设计 / 工具封装哲学 / 并行控制策略 | 高频 | 进阶 |

回答要点：Claude Code 采用 nO 单线程主循环（基于异步生成器实现）+ h2A 异步双缓冲队列 + 受控子代理（I2A，最多 1 分支且禁止递归派生）。Codex 采用标准 ReAct 循环（Think-Tool-Observe）+ Shell 优先执行器，无并行分支设计。两者核心差异在于工具封装层次：Claude Code 用结构化工具（GrepTool/Edit/View），Codex 复用 Unix 原生命令（cat/grep/find）。

关键机制：Claude Code 的主循环通过 while(tool_call) -> execute -> observe -> repeat 模式运行；h2A 实现零延迟路径（基于 Promise）和智能背压控制，吞吐量超 10,000 msg/s。Codex 依赖系统提示词硬编码行为模式，apply_patch 拦截 diff 输出后内部解析。

常见误区：认为两者都是多智能体蜂群架构。实际两者都是单代理设计，区别在于工具封装粒度和并行控制严格程度。

追问：为什么 Claude Code 限制子代理递归派生？h2A 的零延迟路径如何通过 Promise 实现？Codex 的懒加载策略在什么场景下会失效？

区分度：基础层知道都是单代理；进阶层理解工具哲学差异（结构化工具 vs Shell 原生命令）；专家层掌握 h2A 背压控制和异步迭代器实现。

## Post 2

【面试题】两者在文件编辑策略上都采用 diff-centric 理念，但具体实现机制有何不同？为何 diff 编辑能节省 60-80% Token？

| 考察点 | 面试频率 | 难度 |
| --- | --- | --- |
| diff-centric 编辑理念 / Regex vs Shell 搜索 / Token 效率与可审查性 | 中高 | 进阶 |

回答要点：Claude Code 用 GrepTool（基于 Regex 实现）+ Edit 工具；Codex 用 Shell 命令（grep/find）+ apply_patch（生成统一差异格式）。两者都只输出变更部分，不重写整个文件。diff 编辑节省 Token 的原理：模型只关注局部变更，避免传输整文件内容；同时增强可审查性（红绿高亮）并降低幻觉风险。

Token 效率分析：在大型文件编辑场景中，全文输出可能消耗 5k-10k tokens，而 diff 通常仅需 1k-2k tokens。apply_patch 的内部解析流程：CLI 拦截命令 -> 解析 Unified Diff -> 渲染红绿对比 -> 用户审批 -> 应用到文件系统。

常见误区：认为 Codex 会读取并重写整个文件。实际上 apply_patch 只接触变更内容，模型生成的是最小化差异而非全量。

追问：diff 编辑在什么场景下会失效（如大范围重构）？apply_patch 的安全审查流程如何设计？为什么压缩历史中也要保留 diff 格式而非摘要？

区分度：基础层知道用 diff；进阶层理解 Token 效率优势；专家层掌握统一差异格式规范和冲突处理策略。

## Post 3

【面试题】Claude Code 如何解决 LLM 上下文窗口限制？wU2 压缩器的设计思路和触发阈值是什么？为何选择 Markdown 而非向量数据库作为长期存储？

| 考察点 | 面试频率 | 难度 |
| --- | --- | --- |
| 上下文管理 / 三层存储策略 / 重要性评分算法 / 设计哲学 | 中 | 专家 |

回答要点：Claude Code 实现三层架构：短期记忆（当前会话）-> 中期压缩历史 -> 长期存储（CLAUDE.md）。wU2 压缩器在上下文窗口占用 92% 时触发，基于重要性评分算法压缩历史对话，保留约 30% 关键信息。压缩结果注入后续会话，确保模型能追溯任务上下文。

设计哲学：激进简单性——用 Markdown 而非数据库存储，依赖模型自身推理能力而非外部向量检索。这避免了索引维护开销和系统复杂度，保持"白盒"透明度。

重要性评分规则：工具调用及返回值 > 纯对话文本；TODO 状态更新 > 普通输出；用户显式标记最高优先；最近 10 轮对话权重翻倍（时间衰减）；错误和失败操作优先保留。

常见误区：认为压缩是简单文本截断或 LLM 摘要。实际是基于规则+评分的结构化筛选，不调用额外 LLM 避免成本爆炸。

追问：如何量化"重要性"？压缩比 30% 是经验值还是可调参数？为什么不用 RAG 检索替代压缩？评分算法如何平衡代码块和自然语言？

区分度：基础层知道会压缩；进阶层理解 92% 阈值和三层设计；专家层掌握评分算法和 CLAUDE.md 持久化策略。

## Post 4

【面试题】Codex 的懒加载（Lazy Loading）vs Claude Code 的主动扫描（Proactive Scanning），在算法工程师视角下各有什么陷阱？如何设计自适应策略？

| 考察点 | 面试频率 | 难度 |
| --- | --- | --- |
| 上下文加载策略 / 依赖完整性 / 成本控制 / 自适应算法 | 中 | 进阶 |

回答要点：Codex 懒加载仅读取显式请求的文件，依赖 AGENTS.md 全局地图和 Git 历史补充上下文。优点是启动快、Token 消耗低；缺点是在大型 monorepo 中可能遗漏跨模块依赖，需要用户明确指定或配置完善。Claude Code 主动扫描会话启动时扫描项目结构，构建代码库心智模型。优点是对复杂重构更友好，自动发现依赖；缺点是初期 Token 消耗大，可能在巨型项目中触发过早的上下文压缩。

实战选择：外科手术式 bug 修复选 Codex；大规模架构调整选 Claude Code。混合策略是理想方案：启动时轻量扫描（目录树+配置文件），按需深度读取；根据任务复杂度与变更范围触发扫描升级。

自适应策略设计：基于任务类型（单文件修改 vs 跨文件重构）选择加载模式；根据代码库规模（文件数量、平均文件大小）动态调整扫描深度；配合调用链分析增量加载相关依赖。

常见误区：把懒加载当作绝对优势，忽视遗漏导致的幻觉风险。

追问：如何量化依赖遗漏风险？哪些指标（如文件修改失败率、重复修复次数）触发扫描升级？AGENTS.md 的分层继承如何避免配置冲突？

区分度：基础层知道懒加载风险；进阶层能给出补救方案（AGENTS.md + Git）；专家层能提出自适应策略和量化评估指标。

## Post 5

【面试题】两者如何实现沙箱隔离？OS 级隔离（Seatbelt/Docker）与应用层隔离的本质区别是什么？从算法工程师角度分析安全边界。

| 考察点 | 面试频率 | 难度 |
| --- | --- | --- |
| 安全架构 / Seatbelt-Docker 机制 / 权限控制粒度 / 强制访问控制 | 高频 | 进阶 |

回答要点：Codex 采用 OS 级沙箱——macOS 用 Seatbelt 沙箱（限制文件系统访问，仅允许读写指定目录，网络白名单仅含 OpenAI API）；Linux 用 Docker 容器 + iptables 限制网络。这是强制访问控制（MAC），即使模型被攻破也无法逃逸。

Claude Code 采用应用层权限控制——每次文件操作需用户确认（可用 --dangerously-skip-permissions 跳过），设置不持久化。依赖模型自身遵守系统指令，无硬隔离边界。

核心差异：OS 级隔离防止模型被对抗性提示越狱（Prompt Injection）；应用层隔离假设模型善意，仅防意外误操作。前者安全性更高，后者用户体验更流畅但需信任模型对齐。

算法工程师视角：安全设计需要假设最坏情况（模型被攻破或被恶意提示劫持），因此 OS 级隔离更符合纵深防御原则。但成本是降低用户体验和增加部署复杂度。

追问：为什么 Codex 选择 OS 级隔离？Seatbelt 如何精确限制网络访问？Docker 网络隔离的内核机制是什么？如何评估"假设模型善意"的风险？

区分度：基础层知道有沙箱；进阶层理解 Seatbelt/Docker 实现；专家层掌握强制访问控制（MAC）原理和沙箱逃逸防护。

## Post 6

【面试题】显式规划（Explicit Planning）vs 隐式规划（Implicit Planning）：TodoWrite 和 ReAct 循环各有什么优缺点？如何评估规划粒度？

| 考察点 | 面试频率 | 难度 |
| --- | --- | --- |
| 任务规划机制 / 上下文注入策略 / 防遗忘设计 / 规划粒度 | 中 | 进阶 |

回答要点：Claude Code 用 TodoWrite 工具显式展示计划，创建交互式检查清单；每次工具调用后，系统自动注入当前 TODO 状态，防止模型在长对话中迷失目标。Codex 采用隐式规划，通过"尝试-观察-修正"迭代推进，无专门任务列表。

显式规划优点：用户可审查和调整计划，长任务不会失焦；缺点是额外 Token 消耗，过度计划可能延误执行。隐式规划优点：启动快，适合探索式任务；缺点是在复杂多步任务中容易偏离目标，用户难以干预。

关键机制：Claude Code 用全量更新模式而非增量更新，确保 TODO 状态始终一致；/think 模式允许内部推理但不输出到对话，避免干扰用户。

规划粒度评估：过细会导致计划 Token 消耗过大且灵活性不足；过粗会导致模型在执行中迷失方向。最佳实践是按"原子操作单元"划分，每个任务应能在 5-10 分钟内完成。

常见误区：认为显式规划总是更好。实际需要根据任务复杂度选择：简单任务用隐式规划更高效，复杂任务需要显式规划保持可控性。

追问：为什么全量更新而非增量？如何评估规划粒度（过细 vs 过粗）？是否可以混合策略？Todo 状态注入的时机如何选择？

区分度：基础层知道有任务列表；进阶层理解上下文注入机制；专家层掌握防遗忘设计和规划-执行解耦。

## Post 7

【面试题】Claude Code 选择 Regex（正则表达式）而非向量数据库（Vector DB）进行代码搜索，这是什么样的工程权衡？从算法角度分析其利弊。

| 考察点 | 面试频率 | 难度 |
| --- | --- | --- |
| 搜索架构选择 / 计算复杂度权衡 / 维护成本 / 模型能力依赖 | 低 | 专家 |

回答要点：Claude Code 选择 GrepTool（基于 Regex）而非向量数据库，体现"激进的简单性"原则。其核心假设是：当前模型（如 Claude 3.5 Sonnet）已足够聪明，能构建复杂 Regex 模式进行精确搜索。

技术利弊：Regex 优势是无状态、零维护、100% 数据新鲜度（直接搜索当前内容）、精确匹配语法结构；劣势是依赖模型编写正确 Regex、语义搜索能力弱。向量数据库优势是语义理解能力强、模糊匹配；劣势是需维护索引、有更新延迟、增加系统复杂度。

工程权衡：这是"计算套利"——消耗推理时计算资源（让模型花几毫秒思考 Regex），节省维护持久化索引的工程成本。对于代码搜索场景，精确性通常比语义相似度更重要。

适用场景：Regex 适合查找特定函数名、变量引用、API 调用；向量数据库适合理解"找出所有处理用户验证的代码"这类语义查询。

常见误区：认为向量数据库总是更优。实际在代码搜索中，开发者通常知道要找什么（函数名/类名），Regex 的精确匹配更有用。

追问：如何结合两者优势（先用向量库缩小范围，再用 Regex 精确匹配）？Regex 搜索在动态语言（如 Python/JS）中可能遇到什么挑战？如何评估模型编写 Regex 的准确率？

区分度：基础层知道 Claude 用 Regex；进阶层理解"无索引"的优势；专家层掌握计算套利概念和场景化选择策略。

## Post 8

【面试题】CLAUDE.md 与 AGENTS.md 的作用和差异？根据 328 个项目的实证研究，如何编写高质量项目配置文件？

| 考察点 | 面试频率 | 难度 |
| --- | --- | --- |
| 项目长期记忆 / 分层配置 / 生态兼容性 / 实证数据 | 中 | 进阶 |

回答要点：Claude Code 仅支持 CLAUDE.md 单文件配置；Codex 支持 AGENTS.md 分层标准（全局→仓库→子文件夹继承），与 Cursor、Builder.io 等工具生态兼容。基于 328 个项目的实证研究，配置文件常见内容包括：软件架构（72.6%）、开发指南（44.8%）、项目概览（39%）、测试说明（35.4%）、常用命令（33.2%）。

最佳实践："架构+依赖+概述"三元组是高性价比组合（21.6% 项目采用）。架构描述应包含目录结构、模块职责、通信协议；依赖管理需列出关键库及版本；项目概要简述目标用户和核心功能。

分层配置优势：AGENTS.md 支持全局规则在子目录覆盖，允许为 tests/ 文件夹指定不同测试框架。生态兼容性意味着不同 AI 工具可共享同一配置文件。

常见误区：认为配置文件只是简单文档。实际是 AI 代码库的心智模型基础，缺失会导致工具"瞎子摸象"，增加幻觉风险。

追问：为什么架构信息出现率最高？分层配置继承规则如何设计冲突解决？如何平衡详略（过度详细 vs 信息不足）？实证数据显示的"三元组"组合为何是最优解？

区分度：基础层知道配置文件存在；进阶层理解常见内容模式；专家层掌握"三元组+场景化指令+示例代码"结构和分层继承设计。

## Post 9

【面试题】两者在代码审查上的表现差异是什么？为什么 Codex 被认为"实干家"而 Claude Code "理论家"？从产品-模型协同角度分析。

| 考察点 | 面试频率 | 难度 |
| --- | --- | --- |
| GitHub 集成 / 交互式修复 / 审查质量评估 / 产品-模型协同 | 中 | 进阶 |

回答要点：Codex 的 GitHub 应用支持行内评论+交互式修复，能发现"合法的、难以发现的 bug"（如空指针、边界条件），用户可直接点击应用修复。Claude Code 被批评为"verbose without catching obvious bugs"——生成大量文字建议但漏掉明显错误，且缺少与 GitHub 的深度集成。

Builder.io 团队实际使用反馈：GPT-5 的用户满意度比 Claude Sonnet 高 40%。关键差异在于一致性体验（CLI = GitHub UI）和实战导向 vs 理论完美主义。

产品-模型协同效应：Codex 是模型训练方（OpenAI）自研工具，对 diff 格式和 GitHub API 有原生优化；审查结果可直接转化为 PR 评论，形成闭环。Claude Code 优势在大型代码库分析、架构重构建议上更全面。

算法工程师视角：代码审查工具的价值不在于"说得多"而在于"修得好"。交互式修复是关键——发现 bug 后能否一键修复，这才是"实干家"标准。

追问：如何量化代码审查质量（Precision/Recall/F1）？"啰嗦但漏掉明显 bug"的根源是什么（训练数据差异？评估目标不同？）？如何改进 Claude Code 的审查能力？

区分度：基础层知道 Codex 审查更好；进阶层理解交互式修复流程；专家层掌握"实干家 vs 理论家"评价维度和产品-模型协同效应。

## Post 10

【面试题】GPT-5 vs Claude Sonnet/Opus 的成本差异为何如此悬殊？从算法工程师角度分析 Token 效率和推理成本优化策略。

| 考察点 | 面试频率 | 难度 |
| --- | --- | --- |
| Token 效率 / 定价策略 / 使用限制 / 训练方自研优势 / 推理优化 | 中 | 进阶 |

回答要点：GPT-5 成本约为 Sonnet 的一半、Opus 的 1/10。实际使用中，Codex Pro（$20/月）几乎没人报告触碰限制；Claude 在 $17 层级很快用完，$100/$200 层级重度用户仍碰天花板。

效率差异来源：（1）GPT-5 用更激进的量化压缩（如 8-bit 或更低），推理成本更低；（2）模型训练方自研工具，对 diff 格式和上下文管理有针对性优化；（3）Codex 的懒加载策略减少无效 Token 消耗；（4）Shell 优先设计无需额外工具 API 封装层。

算法视角：推理成本主要来自计算量（FLOPs）和内存带宽。量化通过降低权重精度（如 FP32 -> INT8）大幅减少计算和内存开销，但可能损失精度。OpenAI 可能在模型蒸馏和量化上投入更多工程资源。

实战影响：在大型项目中，Token 效率差异会放大 5-10 倍。选择时需考虑使用场景——高频小任务选 Codex，低频大任务选 Claude 可能更合适（但要监控用量）。

追问：为什么 Claude 更容易触碰使用限制（是模型本身还是服务策略）？定价策略背后的用户筛选逻辑是什么？如何评估成本-质量权衡？量化技术如何在保持性能的同时降低推理成本？

区分度：基础层知道 Codex 更便宜；进阶层理解 Token 效率差异；专家层掌握推理成本优化（量化/蒸馏）和产品策略分析。
